{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文内容主要来自：\n",
    "- https://www.jiqizhixin.com/articles/2017-11-02-16\n",
    "- https://en.wikipedia.org/wiki/Combinatorial_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动机\n",
    "神经分类的原始方法是学习单层模型，比如感知机（Rosenblatt, 1958）。但是，将这些方法扩展至多层比较困难，因为硬阈值单元（hard-threshold unit）无法通过梯度下降进行训练，这类单元在几乎所有情况下导数都为 0，且在原点处非连续。因此，研究社区转向带有软激活函数（如 Sigmoid、ReLU）的多层网络，这样梯度可以通过反向传播进行高效计算（Rumelhart et al., 1986）。<br>\n",
    "\n",
    "该方法获得了巨大成功，使研究者使用数百层来训练网络，学得的模型在大量任务上取得非常高的准确率，效果超越之前的所有方法。但是，随着网络变得更深、更宽，出现了一种趋势：使用硬阈值激活函数进行量化，实现二元或低精度推断，可以大幅降低现代深层网络的能耗和计算时间。除量化以外，硬阈值单元的输出规模独立（或者不敏感）于输入规模，这可以缓解梯度消失和爆炸问题，帮助避免使用反向传播进行低精度训练时出现的一些反常现象（Li et al., 2017）。避免这些问题对开发可用于更复杂任务的大型深层网络系统至关重要。<br>\n",
    "\n",
    "## 介绍\n",
    "出于以上原因，我们研究使用硬阈值单元学习深层神经网络的高效技术。我们观察到硬阈值单元输出离散值，这表明组合优化（combinatorial optimization）可能提供训练这些网络的有效方法，因此本论文提出了一种学习深层硬阈值网络的框架。<font color=red>通过为每个隐藏层激活函数指定离散目标集，该网络可分解成多个独立的感知机，每个感知机可以根据输入和目标轻松进行训练。学习深层硬阈值网络的难点在于设置目标，使每个感知机（包括输出单元）要解决的问题是线性可分的，从而达到目标。</font>我们证明使用我们的混合凸组合优化框架（？？？）可以学得这样的网络。<br>\n",
    "\n",
    "基于这一框架，随后我们开发了递归算法，feasible target propagatioin（FTPROP），用来学习深度硬阈值网络。由于这是一个离散优化问题，我们开发了启发法以设置基于每层损失函数的目标。FTPROP 的小批量处理版本可用于解释和证明经常使用的直通的评估器（straight-through estimator/Hinton, 2012; Bengio et al., 2013），现在这可被视为带有每层损失函数和目标启发法的一个特定选择的 FTPROP 实例。最后，我们开发了一个全新的每层损失函数，它能提升深度硬阈值网络的学习能力。我们实际展示了我们的算法在 CIFAR10 的直通评估器为两个卷积网路所带来的提升，以及在 ImageNet 上为带有多个硬阈值激活函数类型的 AlexNet 和 ResNet-18 所带来的提升。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### combinatorial optimization\n",
    "In applied mathematics and theoretical computer science, combinatorial optimization is a topic that consists of finding an optimal object from a finite set of objects.[1] In many such problems, exhaustive search is not feasible. It operates on the domain of those optimization problems, in which the set of feasible solutions is discrete or can be reduced to discrete, and in which the goal is to find the best solution. Some common problems involving combinatorial optimization are the travelling salesman problem (\"TSP\") and the minimum spanning tree problem (\"MST\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py27",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
