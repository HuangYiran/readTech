{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文内容主要来自：\n",
    "- http://alwa.info/2017/03/20/Gensim使用指南/\n",
    "- https://rare-technologies.com/word2vec-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简介\n",
    "Gensim是一个python的包可以自动提取文档语义主题。主要用来对于原始未标记文档进行处理。包括Latent Semantic Analysis, Latent Dirichlet Allocation, Random Projections。这些算法都是unsupervised意味着不需要人工输入。<br>\n",
    "一旦概率模式被发现，原始文档可以有效地进行语义表示，并且可以进行主题查询。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "下面是训练一个Word2Vec型的例子。\n",
    "在定义模型的时候，如果加上训练数据，那么就没有必要再额外调用model.train了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    import argparse\n",
    "    import os\n",
    "    from gensim.models import Word2Vec\n",
    " \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-doc', default = \"./prepronc.en\", help = \"the file that save the training data\")\n",
    "    parser.add_argument('-extrac_data', default = \"./data/\", help = \"the extra data that are used to train the model\")\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "    docus = [line.rstrip('\\n') for line in open(opt.doc)]\n",
    "    texts = [[word for word in document.split()] for document in docus]\n",
    "    model = Word2Vec(texts, size = 500, min_count = 0)\n",
    "    model.save('./english-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec accepts several parameters that affect both training speed and quality.<br>\n",
    "\n",
    "One of them is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:<br>\n",
    "\n",
    "1\n",
    "model = Word2Vec(sentences, min_count=10)  # default value is 5<br>\n",
    "A reasonable value for min_count is between 0-100, depending on the size of your dataset.<br>\n",
    "\n",
    "Another parameter is the size of the NN layers, which correspond to the “degrees” of freedom the training algorithm has:<br>\n",
    "\n",
    "1\n",
    "model = Word2Vec(sentences, size=200)  # default value is 100<br>\n",
    "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds.<br>\n",
    "\n",
    "The last of the major parameters (full list here) is for training parallelization, to speed up training:<br>\n",
    "\n",
    "1\n",
    "model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization<br>\n",
    "The workers parameter has only effect if you have Cython installed. Without Cython, you’ll only be able to use one core because of the GIL (and word2vec training will be miserably slow).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py27",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
