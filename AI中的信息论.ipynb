{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文内容主要来自：\n",
    "- https://www.jiqizhixin.com/articles/01141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息论概述\n",
    "信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。它最初被发明是用来研究在一个含有噪声的信道上用离散的字母表来发送消息，例如通过无线电传输来通信。<br>\n",
    "一般在机器学习中，我们可以将信息论应用在连续型变量上，并使用信息论的一些关键思想来描述概率分布或者量化概率分布之间的相似性。因此在机器学习中，通常要把与随机事件相关信息的期望值进行量化，此外还要量化不同概率分布之间的相似性。在这两种情况下，香农熵都被用来衡量概率分布中的信息内容。香农熵是以信息论之父 Claude Shannon 的名字命名的，也称为信息熵或微分熵（连续）\n",
    "\n",
    "#### 自信息\n",
    "香农熵的基本概念就是所谓的一个事件背后的自信息（self-information），有时候也叫做不确定性。自信息的直觉解释如下，当某个事件（随机变量）的一个不可能的结果出现时，我们就认为它提供了大量的信息。相反地，当观察到一个经常出现的结果时，我们就认为它具有或提供少量的信息。将自信息与一个事件的意外性联系起来是很有帮助的。我们也可以说，均匀分布的熵最大，确定事件的熵最小。<br>\n",
    "基于以上的非正式需求，我们可以找到一个合适的函数来描述自信息，他应该满足下面两个要求:\n",
    "- 取值在 0 到 1 之间的单调递减函数\n",
    "- 独立事件的可加性\n",
    "\n",
    "满足所有这些要求的函数就是负对数，因此我们可以使用负对数表示自信息：$I(P_i) = -log(P_i)$\n",
    "\n",
    "举个例子:<br>\n",
    "我们继续回到简单的硬币抛掷实验中。在信息论中，1bit（也叫做 Shannon）信息代表一次单独硬币抛掷的两种可能结果。相似地，对于两次连续抛掷而言，就需要 4 bit 来描述 4 中可能的结果。通常，用 log_2(n)（2 的对数）bit 来描述 n 个连续的独立随机事件的结果，或者是自信息。下面我们来验证一下一次连续三次的实验中自信息的计算：总共有 2^3=8 种可能的结果，每种结果的概率都是 0.5^3=0.125。所以，这次实验的自信息就是 I（0.125）= -log_2(0.125) = 3。我们需要 3bit 来描述这些所有可能的结果，那么，任何一次连续三次的硬币抛掷的自信息等于 3.0\n",
    "\n",
    "#### 熵\n",
    "到目前为止我们只讨论了自信息。在正常的硬币实验中，自信息实际上都等于香农熵，因为所有的结果都是等概率出现的。通常，香农熵是 X 的所有可能结果的自信息期望值<br>\n",
    "$H(x) = E_{x~P}[I(x)] = -\\sum_{i=1}^np(x_i)I(x_i)= -\\sum_{i=1}^np(x_i)log_b(p(x_i))$<br>\n",
    "如果仔细注意的话，你可能会疑惑，当 p(x_i) = 0 的时候会发生什么，因为这种情况下我们必须计算 0 · log(0)。事实上，我们需要计算的是一个极限：lim_(p→0) p*log(p(x_i))=0。<br>\n",
    "当香农熵泛化到连续域的时候，通常它指的是一种微分熵，对于连续的随机变量 x 及其概率密度函数 p(x)，它的香农熵定义如下：<br>\n",
    "$H(x0) = E_{x~P}[I(x)] = -\\int_{i=1}^np(x_i)I(x_i)= -\\int_{i=1}^np(x_i)log_b(p(x_i))dx$<br>\n",
    "在我们的实验中出现的模式是：越宽广的分布对应着越高的信息熵。\n",
    "\n",
    "#### 交叉熵\n",
    "交叉熵是一个用来比较两个概率分布 p 和 q 的数学工具。它和熵是类似的，我们计算 log(q) 在概率 p 下的期望，而不是反过来：<br>\n",
    "$H(p,q) = E_p[-log(q)]= -\\int_xp(x)log(q(x))dx$<br>\n",
    "- 在信息论中，这个量指的是：如果用「错误」的编码方式 q（而不是 p）去编码服从 p 分布的事件，我们所需要的 bit 数。\n",
    "- 在机器学习中，这是一个衡量概率分布相似性的有用工具，而且经常作为一个损失函数。因为交叉熵等于 KL 散度加上一项信息熵，即 D_KL(p||q) = H(p, q) - H(p)。而当我们针对 Q 最小化交叉熵时，H(p) 为常量，因此它能够被省略。交叉熵在这种情况下也就等价于 KL 散度\n",
    "\n",
    "#### KL 散度\n",
    "与交叉熵紧密相关，KL 散度是另一个在机器学习中用来衡量相似度的量：从 q 到 p 的 KL 散度如下:D_KL(p||q)。在贝叶斯推理中，DKL(p||q) 衡量当你修改了从先验分布 q 到后验分布 p 的信念之后带来的信息增益，或者换句话说，就是用后验分布 q 来近似先验分布 p 的时候造成的信息损失。<br>\n",
    "- 在信息论中, 交叉熵衡量的是用编码方案 q 对服从 p 的事件进行编码时所需 bit 数的平均值，而 KL 散度给出的是使用编码方案 q 而不是最优编码方案 p 时带来的额外 bit 数\n",
    "- 在机器学习中，p 是固定的，交叉熵和 KL 散度之间只相差一个常数可加项，所以从优化的目标来考虑，二者是等价的。而从理论角度而言，考虑 KL 散度仍然是有意义的，KL 散度的一个属性就是，当 p 和 q 相等的时候，它的值为 0。\n",
    "\n",
    "KL 散度有很多有用的性质，最重要的是它是非负的。因为 KL 散度是非负的并且衡量的是两个分布之间的差异，它经常 被用作分布之间的某种距离。然而，它并不是真的距离因为它不是对称的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py27",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
