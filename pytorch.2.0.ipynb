{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文的内容主要来自：\n",
    "- https://www.jiqizhixin.com/articles/2017-08-07-6\n",
    "- http://blog.csdn.net/yangnanhai93/article/details/50127747\n",
    "- http://blog.csdn.net/qq_35571432/article/details/78368797"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量广播\n",
    "当对两个张量进行一些运算的时候，比如加减乘除等， 他们要求进行运算的两个张量的size应该是一致的。但当其大小不符合规则的时候，通过广播就能够自动调整张量的大小。<br>\n",
    "但是其实并不推荐使用这个功能，即使是在很确定广播对张量的调整的情况下。编程的时候，准确的把握定义的张量的形状是很重要的，当需要对张量的大小进行调整的时候，可以使用squeeze，view等方法进行调整，这样既提高了代码的可读性，同时能够更好的抱我对应张量的形状。<br>\n",
    "pytroch的广播和numpy的基本一样，他们都遵守以下规则：\n",
    "1. All input arrays with ndim smaller than the input array of largest ndim, have 1’s prepended to their shapes.\n",
    "2. The size in each dimension of the output shape is the maximum of all the input sizes in that dimension.\n",
    "3. An input can be used in the calculation if its size in a particular dimension either matches the output size in that dimension, or has value exactly 1.\n",
    "4. If an input has a dimension size of 1 in its shape, the first data entry in that dimension will be used for all calculations along that dimension. In other words, the stepping machinery of the ufunc will simply not step along that dimension (the stride will be 0 for that dimension).\n",
    "\n",
    "中文\n",
    "1. 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分都通过在前面加1补齐\n",
    "2. 输出数组的shape是输入数组shape的各个轴上的最大值\n",
    "3. 如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为1时，这个数组能够用来计算，否则出错\n",
    "4. 当输入数组的某个轴的长度为1时，沿着此轴运算时都用此轴上的第一组值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量和变量的高级索引\n",
    "PyTorch 现在支持 NumPy 风格的高级索引的一个子集，这允许用户使用相同的\n",
    "[]风格的操作在张量的每一个维度上选择任意的索引，包括非相邻索引和重复索引；这也使得无需调用 PyTorchIndex[Select, Add, ...]函数即可获得一个更加灵活的索引策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-41 *\n",
       "  4.5838\n",
       "  0.0362\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(5, 5, 5)\n",
    "#纯整数组索引 - 在每个维度上指定任意的索引\n",
    "x[[1, 2], [3, 2], [1, 0]] # 输出(x[1][3][1], x[2][2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 8.6881e-44\n",
       " 4.9130e-32\n",
       " 8.6881e-44\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同样支持广播和复制\n",
    "x[[2, 3, 2], [0], [1]] # 输出(x[2][0][1], x[3][0][1], x[2][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 允许任意的分度器（indexer）形状\n",
    "x[[[1, 0], [0, 1]], [0], [1]].shape # 输出[[x[1][0][1], x[0][0][1]],[x[0][0][1], x[1][0][1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "1.00000e-10 *\n",
       "   0.0000 -4.6566  0.0000 -4.6566  0.0001\n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "\n",
       "(1 ,.,.) = \n",
       "1.00000e-10 *\n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "   0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 2x5x5]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以使用冒号、椭圆\n",
    "x[[0,3], ...] # == x[[0,3], :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以使用张量来索引！\n",
    "y = torch.LongTensor([0, 2, 4])\n",
    "x[y, ...].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择小于 n维，请注意逗号的使用\n",
    "x[[1, 3], ].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高阶梯度\n",
    "现在你可以评估 PyTorch 中的高阶微分。例如，你可以计算 Hessian-Vector积，以模型的梯度的范数为罚项，实现展开的 GAN 和提升的 WGAN 等。在0.2版本中，我们使得所有torch.XXX\n",
    "函数和最流行的n层具备了计算高阶梯度的能力。其余的将会在下一个版本中介绍。<br>\n",
    "下面是一个简短的实例，它以 Resnet-18 模型权重梯度的范数为罚项，因此权重数量的变化比较缓慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bb6cccffdd18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# do an optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torchvision.models import resnet18\n",
    "from torch.autograd import Variable\n",
    "\n",
    "model = resnet18()#.cuda()\n",
    "# dummy inputs for the example\n",
    "input = Variable(torch.randn(2,3,224,224), requires_grad=True)\n",
    "target = Variable(torch.zeros(2).long())#.cuda())\n",
    "# as usual\n",
    "output = model(input)\n",
    "loss = torch.nn.functional.nll_loss(output, target)\n",
    "\n",
    "grad_params = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "# torch.autograd.grad does not accumuate the gradients into the .grad attributes\n",
    "# It instead returns the gradients as Variable tuples.\n",
    "# now compute the 2-norm of the grad_params\n",
    "grad_norm = 0\n",
    "for grad in grad_params:\n",
    "    grad_norm += grad.pow(2).sum()\n",
    "grad_norm = grad_norm.sqrt()\n",
    "# take the gradients wrt grad_norm. backward() will accumulate\n",
    "# the gradients into the .grad attributes\n",
    "grad_norm.backward()\n",
    "# do an optimization step\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们看两个新的概念：\n",
    "- torch.autograd.grad 是一个函数，它包含 [输出、输入列表（为了它你需要梯度）]，并且返回梯度 wrt。这些输入作为元组，而不是将梯度累加到 .grad 属性中。如果你想要进一步在梯度上操作，这很有帮助。\n",
    "- 你可以在梯度上操作，并在其上调用backward()。\n",
    "\n",
    "支持高阶梯度的n层列表是：<br>\n",
    "AvgPool*d、 BatchNorm*d、 Conv*d、MaxPool1d,2d、Linear、 Bilinear\n",
    "pad、ConstantPad2d、ZeroPad2d、LPPool2d、PixelShuffle\n",
    "ReLU6、LeakyReLU、PReLU、Tanh、Tanhshrink、Threshold、Sigmoid、HardTanh、ELU、Softsign、SeLU\n",
    "L1Loss、NLLLoss、 PoissonNLLLoss、LogSoftmax、Softmax2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布式PyTorch\n",
    "torch.distributed包允许我们在多机器间交换Tensors。使用该软件包，我们能够将神经网络放在多机器和使用较大批量进行训练。<br>\n",
    "该distributed包遵循MPI风格的程序设计模型。这意味着该软件包会提供像send、recv、all_reduce那样的函数以允许在结点（机器）之间交换Tensors。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ###### 这部分并不熟悉，就先不深入了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新特征\n",
    "引入forward_pre_hook，以在前向函数（forward function）被调用之前执行用户指定的闭包。<br>\n",
    "易于获得非叶梯度（non-leaf gradient）：目前，我们必须使用hooks获取和检查中间值的梯度。这种做法对于简单的检查并不方便。因此，我们引入了retain_grad。以下示例可充分解释该方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9770  0.7280  3.5791\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.rand(1, 3), requires_grad=True)\n",
    "h1 = input * 3\n",
    "out = (h1 * h1).sum()\n",
    "\n",
    "h1.retain_grad()\n",
    "out.backward()\n",
    "print(h1.grad)# without calling retain_grad(), h1.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新型层\n",
    "这里面的大部分表示都没有用过\n",
    "- 使用F.grid_sample和F.affine_grid的空间变换网络<br>\n",
    "论文《自正则化神经网络（Self-Normalizing Neural Networks）》提出nn.SeLU和nn.AlphaDropout。论文《从卷积序列到序列学习（Convolutional Sequence to Sequence Learning）》提出nn.GLU（线性门控单元）。<br>\n",
    "- 通过torch.utils.weight_norm实现权值归一化（Weight Normalization）。<br>\n",
    "- 计算cross_entropy_loss和nll_loss时，可以使用ignore_index参数来忽略特定的目标索引（target indice）。这是实现掩码的一种廉价、有用的方式，你可以获取计算损失时所忽略的mask指数。\n",
    "- F.normalize实现各维度的重归一化。\n",
    "- F.upsample和nn.Upsample将多个上采样层合并成一个函数。该函数可实现第二次和第三次双线性／三线性／最近上采样（bilinear/trilinear/nearest upsampling）。\n",
    "- nn.EmbeddingBag：在构建词袋模型（bag-of-words model）时，在Sum或Mean之后执行Embedding是一种常见做法。对于不同长度的序列，计算词袋嵌入涉及到掩码。我们提供一个单独的nn.EmbeddingBag，它能够更高效、快捷地计算词袋嵌入，尤其是不同长度的序列。\n",
    "- 使用bce_with_logits的数值稳定的二元交叉熵参数（Binary Cross-Entropy loss）。\n",
    "- 使用PoissonNLLLoss的带有目标泊松分布的负对数似然损失。\n",
    "- cosine_similarity：沿维度计算并返回x1和x2之间的余弦相似度（cosine similarity）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练工具\n",
    "学习率调度器：torch.optim.lr_scheduler提供多个简单或聪明的方法来调整当前的学习率。在实验过程中，这些方法都很方便，i为用户可能想要做的事提供代理。<br>\n",
    "提供多种策略，可根据具体情况使用，详见\n",
    "- http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate：\n",
    "- ReduceLROnPlateau, LambdaLR, StepLR, MultiStepLR, ExponentialLR\n",
    "\n",
    "ConcatDataset是一种可以合并和连接两个单独的数据集的数据集元类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 和 autograd 中的新特性\n",
    "- 现在，所有reduce函数如sum和mean默认为挤压降维。例如，\n",
    "torch.sum(torch.randn(10, 20))返回一个1D 张量。\n",
    "- x.shape，与numpy类似。一种等价于x.size()的便捷属性。\n",
    "- torch.matmul，与np.matmul类似。\n",
    "- bitwise and、or、xor、lshift、rshift对inverse、gesv、cumprod、atan2的autograd支持\n",
    "- 通过关键字参数选项（keyword argument option）可获取无偏var和std。\n",
    "- torch.scatter_add - torch.scatter，除了重复指数的情况，这些值都可以汇总。\n",
    "- 无给定参数时，torch.median与torch.sum类似，即它减少所有维度，并返回扁平张量（flattened Tensor）的单个median值。\n",
    "- masked_copy_被重命名为masked_scatter_（因为对masked_copy_有反对声）。\n",
    "- torch.manual_seed现在也对所有的CUDA设备播种。\n",
    "- 你现在可以通过关键字参数torch.rand(1000, generator=gen)\n",
    "指定随机数生成器对象（random number generator object）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py27",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
