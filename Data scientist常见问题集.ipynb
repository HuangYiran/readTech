{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本文内容主要来自：\n",
    "- https://www.datasciencecentral.com/profiles/blogs/25-questions-to-detect-fake-data-scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the life cycle of a data science project?\n",
    "- 需求分析\n",
    "- 数据搜集\n",
    "- 4C\n",
    "- 建模\n",
    "- 提交"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do you measure yield (over base line) resulting from a new or refined algorithm or architecture?\n",
    "这就复杂了，往大的说有：\n",
    "- 精确性与最优性。基于正确性基础上\n",
    "- 计算复杂度，时间成本\n",
    "- 适应性。适应变化的输入和各种数据类型\n",
    "- 可移植性\n",
    "- 鲁棒性\n",
    "\n",
    "而针对机器学习模型，一般分为下面两大类，在Scikit中基本上都可以找到对应的方法：\n",
    "- 对于分类器，或者说分类算法，评价指标主要有accuracy， [precision，recall，宏平均和微平均，F-score，pr曲线]，ROC-AUC曲线，gini系数。\n",
    "- 对于回归分析，主要有mse和r2/拟合优度\n",
    "\n",
    "下面是具体的介绍：\n",
    "针对二分问题：\n",
    "- 构建混淆矩阵(confusion metrix, 第一个字母表示判断的正误，第二个字母表示预测的结果)\n",
    "- 准确率 precision P = $\\frac{TP}{TP + FP}$ \n",
    "- 召回率(or True Positive Rate) recall R = $\\frac{TP}{TP + FN}$\n",
    "- False positive rate FPR = $\\frac{FP}{FP + TN}$ \n",
    "- F值 F = $\\frac{2 * P * R}{P + R}$\n",
    "- G-mean也能评价不平衡数据的模型表现 G-mean = $\\sqrt{\\frac{TP}{TP + FN}*\\frac{TN}{TN + FP}}$\n",
    "\n",
    "如果只有一个二分类混淆矩阵，那么用以上的指标就可以进行评价，没有什么争议，但是当我们在n个二分类混淆矩阵上要综合考察评价指标的时候就会用到宏平均和微平均。宏平均（macro-average）和微平均（micro-average）是衡量文本分类器的指标。\n",
    "- macro-average 是先对每一个类统计指标值，然后在对所有类求算术平均值。宏平均指标相对微平均指标而言受小类别的影响更大。\n",
    "    - macro_P = $\\frac{1}{n}\\sum^{n}_{i = 1}P_i$\n",
    "    - macro_R = $\\frac{1}{n}\\sum^{n}_{i = 1}R_i$\n",
    "    - macro_F = $\\frac{2 * macro\\_P * macro\\_R}{macro\\_P + macro\\_R}$\n",
    "    - or macro_F = $\\frac{1}{n}\\sum^{n}_{i = 1}F_i$\n",
    "- Micro-averaging 是对数据集中的每一个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标\n",
    "    - micro_P = $\\frac{\\sum^{n}_{i = 1}TP_i}{\\sum^{n}_{i = 1}TP_i + \\sum^{n}_{i = 1}FP_i}$\n",
    "    - micro_R = $\\frac{\\sum^{n}_{i = 1}TP_i}{\\sum^{n}_{i = 1}TP_i + \\sum^{n}_{i = 1}NF_i}$\n",
    "    - micro_F = $\\frac{2 * macro\\_P * macro\\_R}{macro\\_P + macro\\_R}$\n",
    "    \n",
    "ROC\n",
    "- Receiver Operating Characteristic曲线和AUC(Area Under Curve)常被用来评价一个二值分类器（binary classifier）的优劣\n",
    "- AUC有一个独特的优势，就是不关注具体得分，只关注排序结果，这使得它特别适用于排序问题的效果评估。AUC这个指标有两种解释方法，一种是传统的“曲线下面积”解释，另一种是关于排序能力的解释。例如0.7的AUC，其含义可以大概理解为：给定一个正样本和一个负样本，在70%的情况下，模型对正样本的打分高于对负样本的打分。可以看出在这个解释下，我们关心的只有正负样本之间的分数高低，而具体的分值则无关紧要\n",
    "- ROC曲线的横坐标为false positive rate（FPR），纵坐标为true positive rate（TPR）（也就是recall）\n",
    "- .AUC一般评分标准\n",
    "    - 90-1 = very good (A)\n",
    "    - .80-.90 = good (B)\n",
    "    - .70-.80 = not so good (C)\n",
    "    - .60-.70 = poor (D)\n",
    "    - .50-.60 = fail (F)\n",
    "    \n",
    "Gini\n",
    "- 基尼系数是指国际上通用的、用以衡量一个国家或地区居民收入差距的常用指标。基尼系数介于0-1之间，基尼系数越大，表示不平等程度越高\n",
    "- Gini coefficient 是指绝对公平线(line of equality)和洛伦茨曲线(Lorenz Curve)围成的面积与绝对公平线以下面积的比例\n",
    "- 洛伦兹曲线（Lorenz curve），也译为“劳伦兹曲线”。就是，在一个总体（国家、地区）内，以“最贫穷的人口计算起一直到最富有人口”的人口百分比对应各个人口百分比的收入百分比的点组成的曲\n",
    "- $gini = A / (A + B) = (AUC - C) / (A + B) = (AUC -0.5) / 0.5 = 2*AUC - 1$\n",
    "\n",
    "下面是回归模型评估\n",
    "MSE\n",
    "- $\\frac{1}{n}\\sum(\\bar{y}-y_i)^2 = Var(Y)$\n",
    "\n",
    "确定系数R^2（coefficient of determination）\n",
    "- $1 - \\frac{Var(\\varepsilon)}{Var(Y)}$ \n",
    "- 即(1 \\- 预测模型的mse/数据本身的mse) （数据本身的mse就是直接将数据label均值作为预测的mse）\n",
    "- 表示的是有解释变量情况下的均方误差与没有解释变量情况下的均方误差的比值，也即不能被模型解释的均方误差占总的均方误差的比例。这样R2表示的就是能被模型解释的变异性的比例\n",
    "- R^2最大值为1。R^2的值越接近1，说明回归直线对观测值的拟合程度越好；反之，R^2的值越小，说明回归直线对观测值的拟合程度越差\n",
    "\n",
    "to be continue..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is cross-validation? How to do it right?\n",
    "交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is it better to design robust or accurate algorithms?\n",
    "鲁棒性？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Have you written production code? Prototyped an algorithm? Created a proof of concept?\n",
    "？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the biggest data set you have worked with, in terms of training set size, and in terms of having your algorithm implemented in production mode to process billions of transactions per day / month / year?\n",
    "介绍大数据经历？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Name a few famous API's (for instance Google search). How would you create one?\n",
    "Vim, Youtube, Amazon???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to efficiently scrape web data, or collect tons of tweets?\n",
    "这个得补补BeautifulSoup，Request, Cookie??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to optimize algorithms (parallel processing and/or faster algorithm: provide examples for both)\n",
    "？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples of NoSQL architecture?\n",
    "XML？？？HTML???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  How do you clean data?\n",
    "Completting, Correcting, Converting???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do you define / select metrics? Have you designed and used compound metrics?\n",
    "compound metrics？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples of bad and good visualizations?\n",
    "bad\n",
    "- visualize the data before cleaning\n",
    "- choose the wrong visualization, for example, use pie to show the number of feature.\n",
    "- too much informations in one visualization, \n",
    "- misrepresentation of data：纵坐标是从48而不是0开始的，这个图容易让让人以为16年的数值是12年的两倍，但其实并不是这样的。For instance, this bar chart seems to show that the percentage of women covered under a job guarantee scheme more than doubled from 2012-13 to 2016-17. However, when we look at the y-axis, we see that it begins from 48%, not 0%. This misrepresents the marginal improvement of around 5.5% as a 2x increase.\n",
    "- Inconsistent Scales\n",
    "[misrepresentaion of data](https://raw.githubusercontent.com/HuangYiran/readTech/master/fotos/misrepresentation_of_data.png)\n",
    "\n",
    "good\n",
    "- Provide Content\n",
    "- Use the right charts\n",
    "- Less is More: Perfection is Achieved Not When There Is Nothing More to Add, But When There Is Nothing Left to Take Away\n",
    "- Visualize Missing Data\n",
    "- Reduce Cognitive Overload\n",
    "    - Sort your Data\n",
    "    - Do the Calculations\n",
    "    - Identify the Relationships\n",
    "    - Accurate Scaling\n",
    "    - Color your Data Viz, \n",
    "        - Use contrasting or complementary colors for categorical data (gender, region) to compare and highlight the differences\n",
    "        - Use analogous colors while grouping related data like product type\n",
    "        - Use a gradient from the same color palette to denote degree of progression \n",
    "        - Be consistent across your visualizations e.g. use the same color to represent your source/medium data across all your charts\n",
    "- Empower the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Have you been involved - as an adviser or architect - in the design of dashboard or alarm systems?\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How frequently an algorithm must be updated? What about lookup tables in real-time systems?\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Provide examples of machine-to-machine communication.\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Provide examples where you automated a repetitive analytical task.\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do you assess the statistical significance of an insight?\n",
    "?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to turn unstructured data into structured data?\n",
    "STORED？？？？？一般应该手动进行分析，确定哪些信息是你想要的，然后把这些信息提取出来，用关系模型进行存储。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to very efficiently cluster 100 billion web pages, for instance with a tagging or indexing algorithm? \n",
    "？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If you were interviewing a data scientist, what questions would you ask her?\n",
    "？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain what regularization is and why it is useful. What are the benefits and drawbacks of specific methods, such as ridge regression and LASSO?\n",
    "against overfitting, ridge regression = RSS + L2？？？LASSO = RSS + L1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain what a local optimum is and why it is important in a specific context, such as k-means clustering. What are specific ways for determining if you have a local optimum problem? What can be done to avoid local optima?\n",
    "针对NP问题，我们一般很难得到他的最优解，这时候，我们会选择使用局部搜索的方法，来获取问题的局部最优解。\n",
    "momentum, 为了使局部最优逼近全局最优，除了使用全局搜索的算法，就只能靠多尝试了。？？？？？？\n",
    "在做爬山的时候，一般不是很担心局部最优的问题，因为，想同时，在多个不同的维度上，达到最小值，是很难得，一般在全局最优出才会出现这种情况？？？，我们应该考虑的是，应该如何防止跳过全局最优，这时候momentum就出场了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assume you need to generate a predictive model of a quantitative outcome variable using multiple regression. Explain how you intend to validate this model.\n",
    "--前面 regression的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain what precision and recall are. How do they relate to the ROC curve?\n",
    "-- 前面有解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain what a long tailed distribution is and provide three examples of relevant phenomena that have long tails. Why are they important in classification and prediction problems?\n",
    "在一個累積分布函數中，一個随机变量 X　的分布，出現以下狀況時，被稱為是一個長尾分布。假設對所有t > 0：$lim_{x->inf.}Pr[X>x+t|X>x] = 1$\n",
    "http://mockinterview.co/index.php/question/explain-what-a-long-tailed-distribution-is-and-provide-three-examples-of-relevant-phenomena-that-have-long-tails-why-are-they-important-in-classification-and-regression-problems/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is latent semantic indexing? What is it used for? What are the specific limitations of the method?\n",
    "？？？Latent semantic indexing, sometimes referred to as latent semantic analysis, is a mathematical method developed in the late 1980s to improve the accuracy of information retrieval. It uses a technique called singular value decomposition to scan unstructured data within documents and identify relationships between the concepts contained therein.\n",
    "\n",
    "In essence, it finds the hidden (latent) relationships between words (semantics) in order to improve information understanding (indexing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the Central Limit Theorem? Explain it. Why is it important? When does it fail to hold?\n",
    "？？？？中心极限定理说明，在适当的条件下，大量相互独立随机变量的均值经适当标准化后依分布收敛于正态分布。这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量之和近似服从正态分布的条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain what resampling methods are and why they are useful. Also explain their limitations.\n",
    "？？？？？？？？？null hypothese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain the differences between artificial neural networks with softmax activation, logistic regression, and the maximum entropy classifier.\n",
    "待补"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain selection bias (with regards to a dataset, not variable selection). Why is it important? How can data management procedures such as missing data handling make it worse?\n",
    "？？？选择性偏差溯源英文为Selection Bias，指的是在研究过程中因样本选择的非随机性而导致得到的结论存在偏差,也称选择性偏差为选择性效应（Selection Effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Provide a simple example of how an experimental design can help answer a question about behavior. For instance, explain how an experimental design can be used to optimize a web page. How does experimental data contrast with observational data.\n",
    "？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain the difference between \"long\" and \"wide\" format data. Why would you use one or the other?\n",
    "好像是指数据集的有样子？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is mean imputation of missing data acceptable practice? Why or why not?\n",
    "- Mean imputation reduces the variance of the imputed variables.\n",
    "- Mean imputation shrinks standard errors, which invalidates most hypothesis tests and the calculation of confidence interval.\n",
    "- Mean imputation does not preserve relationships between variables such as correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain Edward Tufte's concept of \"chart junk.\" \n",
    "？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is an outlier? Explain how you might screen for outliers and what you would do if you found them in your dataset. Also, explain what an inlier is and how you might screen for them and what you would do if you found them in your dataset.\n",
    "待补"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is principal components analysis (PCA)? Explain the sorts of problems you would use PCA for. Also explain its limitations as a method.\n",
    "待补"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You have data on the duration of calls to a call center. Generate a plan for how you would code and analyze these data. Explain a plausible scenario for what the distribution of these durations might look like. How could you test (even graphically) whether your expectations are borne out?\n",
    "？？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain what a false positive and a false negative are. Why is it important to differentiate these from each other? Provide examples of situations where (1) false positives are more important than false negatives, (2) false negatives are more important than false positives, and (3) these two types of errors are about equally important.\n",
    "1. 事故预测\n",
    "2. 待补"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain likely differences encountered between administrative datasets and datasets gathered from experimental studies. What are likely problems encountered with administrative data? How do experimental methods help alleviate these problems? What problems do they bring?\n",
    "？？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "What is a gold standard ? \n",
    "Believe it or not there are data scientists (even at very senior levels) who claim to know a hell lot about supervised machine learning and know nothing about what a gold standard is!\n",
    "What is the difference between supervised learning and unsupervised learning? - Give concrete examples.\n",
    "What does NLP stand for?\n",
    "Some data scientists claim to also do NLP.  \n",
    "Write code to count the number of words in a document using any programming language. Now, extend this for bi-grams.\n",
    "I have seen a senior level data scientist who actually struggled to implement this. \n",
    "What are feature vectors?\n",
    "When would you use SVMs vs Random Forrest and Why?\n",
    "What is your definition of Big Data, and what is the largest size of data you have worked with? Did you parallelize your code?\n",
    "If their notion of big data is just volume - you may have a problem. Big Data is more than just volume of data. If the largest size of data they have worked with is 5MB - again you may have a problem.\n",
    "How do you work with large data sets?\n",
    "If the answer only comes out as hadoop it clearly shows that their view of solving problems is extremely narrow. Large data problems can be solved with:\n",
    "1. efficient algorithms\n",
    "2. multi-threaded applications\n",
    "3. distributed programming\n",
    "4. more...\n",
    "Write a mapper function to count word frequencies (even if its just pseudo code)\n",
    "Write a reducer function for counting word frequencies (even if its just pseudo code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF, SVM\n",
    "Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class. For multiclass problem you will need to reduce it into multiple binary classification problems.<br>\n",
    "Random Forest works well with a mixture of numerical and categorical features. When features are on the various scales, it is also fine. Roughly speaking, with Random Forest you can use data as they are. SVM maximizes the \"margin\" and thus relies on the concept of \"distance\" between different points. It is up to you to decide if \"distance\" is meaningful. As a consequence, one-hot encoding for categorical features is a must-do. Further, min-max or other scaling is highly recommended at preprocessing step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "data_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
