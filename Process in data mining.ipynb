{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义问题并收集数据\n",
    "kaggle第一题，泰坦尼克号幸存预测\n",
    "\n",
    "### 数据预处理\n",
    "##### 实用的包\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:05:08) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named pandas",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a72a6bb5715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Python version: {}\"\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m#collection of functions for data processing and analysis modeled after R dataframes with SQL like features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pandas version: {}\"\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pandas"
     ]
    }
   ],
   "source": [
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 观察数据\n",
    "- 首先我们要先读入数据，然后用info和sample方法来大概了解一下数据。\n",
    "```\n",
    "data_raw = pd.read_csv('../input/train.csv')\n",
    "data_val  = pd.read_csv('../input/test.csv')\n",
    "#remember python assignment or equal passes by reference vs values, so we use the copy function: \n",
    "data1 = data_raw.copy(deep = True)\n",
    "data_cleaner = [data1, data_val]\n",
    "#preview data\n",
    "print (data_raw.info()) # 包含简单的列信息和统计信息\n",
    "#data_raw.head()\n",
    "#data_raw.tail()\n",
    "data_raw.sample()\n",
    "```\n",
    "\n",
    "具体了解每个属性的类型，取值，及可能包含的信息\n",
    "需要注意的是，用于预测的属性并不是越多越好的。像用户ID和船票这些是属于随机属性，所以应该从分析列表中移除。\n",
    "##### 4'C 数据清理\n",
    "- correcting，重新观察数据，看是否存在异常或潜在异常的数据。对于明显的错误数据，如年龄是负的，就可以直接处理掉了，而对于还不好确定的，就先留个心眼，等后续进一步分析后，再做定夺。\n",
    "- completing, 处理null值和空值，一些算法并不能自动处理这些情况，所以我们应该事先把他们处理掉。虽然也存在可以处理这些情况的模型，比如决策树，但一般我们会更愿意对比多个不同的模型。直接删除是比较不推荐的一个做法，一般我们可以：A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value.\n",
    "- creating，通过已经存在的属性来创建新的属性，并希望这个属性能为预测结果带来新的信息。\n",
    "- converting, 在python中并没有日期和货币类型，为了方便处理，我们一般会把这些类型转换成我们能够处理的类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')\n",
    "```\n",
    "\n",
    "通过上面的代码，我们可以知道哪里有缺失值，下面是clean数据的一些具体方法；\n",
    "```\n",
    "##COMPLETING: complete or delete missing values in train and test/validation dataset\n",
    "for dataset in data_cleaner:    \n",
    "    #complete missing age with median\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "\n",
    "    #complete embarked with mode\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "\n",
    "    #complete missing fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#delete the cabin feature/column and others previously stated to exclude in train dataset\n",
    "\n",
    "drop_column = ['PassengerId','Cabin', 'Ticket']\n",
    "\n",
    "data1.drop(drop_column, axis=1, inplace = True)\n",
    "\n",
    "print(data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print(data_val.isnull().sum())\n",
    "```\n",
    "\n",
    "然后是通过原有属性创建新属性的一些具体实现：\n",
    "```\n",
    "dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n",
    "dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n",
    "dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "```\n",
    "接下来是把categorical数据转换成容易处理的数据类型，确定哪些是应变量哪些是自变量。可以使用的方法有很多，比如Categorical Encoding，Sklearn LabelEncoder，Sklearn OneHotEncoder，Pandas Categorical dtype，pandas.get_dummies\n",
    "```\n",
    "label = LabelEncoder()\n",
    "dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
    "dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
    "dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
    "dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n",
    "\n",
    "#define y variable aka target/outcome\n",
    "Target = ['Survived']\n",
    "\n",
    "#define x variables for original features aka feature selection\n",
    "data1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\n",
    "data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\n",
    "data1_xy =  Target + data1_x\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "\n",
    "#define x variables for original w/bin features to remove continuous variables\n",
    "data1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "#define x and y variables for dummy features original\n",
    "data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
    "```\n",
    "在结束这一阶段之前，再次检查一遍，数据是否已经清洗完成\n",
    "```\n",
    "print('Train columns with null values: \\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data1.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values: \\n', data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print (data_val.info())\n",
    "print(\"-\"*10)\n",
    "\n",
    "data_raw.describe(include = 'all')\n",
    "```\n",
    "\n",
    "从训练数据集中，划分为训练和测试两部分，以防止模型的Overfitting。\n",
    "```\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n",
    "\n",
    "print(\"Data1 Shape: {}\".format(data1.shape))\n",
    "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_x.shape))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计分析数据\n",
    "通过画表和统计计量来进一步分析数据之间的关系\n",
    "```\n",
    "# 分析不同属性，存活率。比如针对性别，分别列出男性和女性的存活率\n",
    "for x in data1_x:\n",
    "    if data1[x].dtype != 'float64' :\n",
    "        print('Survival Correlation by:', x)\n",
    "        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
    "        print('-'*10, '\\n')\n",
    "```\n",
    "\n",
    "重要事情加重强调，IMPORTANT: Intentionally plotted different ways for learning purposes only. \n",
    "- optional plotting w/pandas: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "- we will use matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "- to organize our graphics will use figure:https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n",
    "- subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n",
    "- and subplotS: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html?highlight=matplotlib%20pyplot%20subplots#matplotlib.pyplot.subplots\n",
    "- 详细见这个链接 https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/notebook\n",
    "```\n",
    "#pair plots of entire dataset\n",
    "pp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
    "pp.set(xticklabels=[])\n",
    "\n",
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data1)\n",
    "```\n",
    "??????具体得到了什么结论也并无说明？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建模\n",
    "首先应该确定问题类型，是有监督，无监督还是强化的。是拟合还是分类。再之后就是模型选择了。事实上目前并没有一个在所有情况下普遍使用的最好的模型，所以我们最好的方法就是不断尝试，调节然后对比。那么现成的模型都有哪些呢：<br>\n",
    "Machine Learning Selection:\n",
    "- Sklearn Estimator Overview http://scikit-learn.org/stable/user_guide.html\n",
    "- Sklearn Estimator Detail http://scikit-learn.org/stable/modules/classes.html\n",
    "- Choosing Estimator Mind Map http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "- Choosing Estimator Cheat Sheet https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf\n",
    "\n",
    "Machine Learning Classification Algorithms:\n",
    "- Ensemble Methods\n",
    "- Generalized Linear Models (GLM)\n",
    "- Naive Bayes\n",
    "- Nearest Neighbors\n",
    "- Support Vector Machines (SVM)\n",
    "- Decision Trees\n",
    "- Discriminant Analysis\n",
    "\n",
    "```\n",
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]\n",
    "\n",
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = data1[Target]\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1[data1_x_bin], data1[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "    \n",
    "    row_index+=1\n",
    "\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict\n",
    "\n",
    "# 最后为了直观，把图画一下吧\n",
    "#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n",
    "\n",
    "#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析模型结果\n",
    "通过上面简单的数据清理，分离和建模之后，我们可以得到一个还不错的结果，但是一个永恒的问题是，我们能不能做的更好呢？（另外一个问题是，是否值得花时间去进行优化呢？）<br>\n",
    "##### 设定baseline\n",
    "在我们进一步优化我们的模型之前，我们应该找到一个方法来判别一个模型是否应该进行保留。所以我们应该取一个baseline，从而排除比这个baseline还差模型。以binary问题为例，在没有任何信息的情况下，我们有50%的回答正确的概率。这就可以做为一个baseline。但一般具体的问题我们不会完全没有一点信息，以这个泰坦尼克号的幸存为例，如果我们回答所有人都幸存下来，那么我们的正确率是62%。也就是说，在不需要做任何处理的情况下，我们可以得到的正确率是62%，那么我们会希望在加入模型之后，我们的结果会比这个更好，所以，针对这个问题，更建议的做法是，把baseline设为62%\n",
    "##### 建立自己的决策树\n",
    "并不清楚，插入这节的意义？？？？？\n",
    "##### 使用cv\n",
    "##### 对模型的超参进行微调\n",
    "前面我们再使用模型的时候，使用的都是默认的参数，为了获得更好的结果，我们应该尝试对比不同的参数取值。为了使选值更具有目的性，我们必须清楚各个参数具体的作用是什么，这也就要求我们对我们使用的模型有一定的了解。所以我们打一个略字在下面，表示自己回家好好看看。<br>\n",
    "略<br>\n",
    "下面我们使用 ParameterGrid, GridSearchCV, and customized sklearn scoring来对模型进行微调，并使用graphviz来进行可视化。（当然也可以用hyperopt）\n",
    "```\n",
    "略\n",
    "```\n",
    "##### 通过参数选择来调节模型\n",
    "正如前面提到的，属性并不是多多益善的，关键还是得看其使用价值。 Sklearn提供了集中属性的选择方法, 下面我们使用recursive feature elimination (RFE) with cross validation (CV)来进行属性选择.\n",
    "```\n",
    "#base model\n",
    "print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \n",
    "print('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n",
    "\n",
    "print(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n",
    "print(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n",
    "print(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "\n",
    "#feature selection\n",
    "dtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\n",
    "dtree_rfe.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "#transform x&y to reduced features and fit new model\n",
    "#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "X_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\n",
    "rfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split)\n",
    "\n",
    "#print(dtree_rfe.grid_scores_)\n",
    "print('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \n",
    "print('AFTER DT RFE Training Columns New: ', X_rfe)\n",
    "\n",
    "print(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n",
    "print(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n",
    "print(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#tune rfe model\n",
    "rfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "rfe_tune_model.fit(data1[X_rfe], data1[Target])\n",
    "\n",
    "#print(rfe_tune_model.cv_results_.keys())\n",
    "#print(rfe_tune_model.cv_results_['params'])\n",
    "print('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n",
    "#print(rfe_tune_model.cv_results_['mean_train_score'])\n",
    "print(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n",
    "#print(rfe_tune_model.cv_results_['mean_test_score'])\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n",
    "print(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "# 可视化\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
    "                                feature_names = data1_x_bin, class_names = True,\n",
    "                                filled = True, rounded = True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现\n",
    "最后我们应该开始考虑提交结果的事情了。那么在我们可以使用多个不同的模型的情况下，为什么我们要选择单一模型，而不进行投票制呢？？？这也是我们的基本思想。我们分析不同模型的预测结果，计算他们的correlation。如果两个模型预测的结果的相关系数很高，那么我们可以二者取其一，反之，那么我们最后两个都利用起来。\n",
    "```\n",
    "#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n",
    "#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\n",
    "correlation_heatmap(MLA_predict)\n",
    "\n",
    "# 去掉相关系数为1的模型\n",
    "vote_est = [\n",
    "    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc',ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "\n",
    "    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n",
    "    ('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    \n",
    "    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "    ('lr', linear_model.LogisticRegressionCV()),\n",
    "    \n",
    "    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    \n",
    "    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    \n",
    "    #SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "   ('xgb', XGBClassifier())\n",
    "\n",
    "]\n",
    "\n",
    "# 两种不同的投票方式\n",
    "#Hard Vote or majority rules\n",
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "vote_hard.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \n",
    "print(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "#Soft Vote or weighted probabilities\n",
    "vote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n",
    "vote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\n",
    "vote_soft.fit(data1[data1_x_bin], data1[Target])\n",
    "\n",
    "print(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \n",
    "print(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\n",
    "print('-'*10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py27",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
